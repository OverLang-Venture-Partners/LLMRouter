# GMTRouter Training Configuration
# Based on: https://github.com/ulab-uiuc/GMTRouter
#
# IMPORTANT: For actual training, please use the original GMTRouter repository
# This config is for reference and inference integration with LLMRouter

# Dataset configuration
dataset:
  name: mt_bench                    # Dataset: chatbot_arena, gsm8k, mmlu, mt_bench
  path: ./data                      # Path to data root (download from Google Drive)

# Data preprocessing
config:
  preprocess: true                  # Whether to preprocess JSONL to .pt format

# Training configuration
train:
  id: llmrouter_gmt                 # Experiment identifier
  epochs: 350                       # Number of training epochs
  lr: 5e-4                          # Learning rate (5e-4 recommended)
  record_per_user: 10               # Minimum interactions per user
  prediction_count: 256             # Number of predictions per batch
  multi_turn: false                 # Enable multi-turn conversation mode
  aggregation_type: mean            # Aggregation: mean, max, attention
  objective: auc                    # Objective: auc or accuracy
  binary: true                      # Binary classification (pairwise comparison)
  eval_every: 5                     # Evaluate every N epochs
  seed: 136                         # Random seed for reproducibility

# Checkpoint configuration
checkpoint:
  root: ./models                    # Checkpoint save directory
  save_every: 25                    # Save checkpoint every N epochs

# GMTRouter-specific model configuration
gmt_config:
  # Graph neural network architecture
  num_gnn_layers: 2                 # Number of HGT (Heterogeneous Graph Transformer) layers
                                    # 2 for single-turn, 3 for multi-turn
  hidden_dim: 128                   # Hidden dimension for GNN embeddings
  dropout: 0.1                      # Dropout rate for regularization

  # Personalization
  personalization: true             # Enable user preference learning
  record_per_user: 10               # Minimum records per user to learn preferences

  # Node types (fixed in GMTRouter architecture)
  # - User: User preference embeddings (learned)
  # - Session: Conversation session representations (learned)
  # - Query: Query text embeddings (from PLM)
  # - LLM: Model embeddings (from PLM)
  # - Response: Response quality (rating-based)

  # Edge types: 21 types in HeteroEdges enum
  # Including: own, owned_by, answered_by, answered_to, next, prev, etc.

# Data paths (for LLMRouter integration)
data_path:
  data_root: ./data                 # Root directory for GMTRouter data
  training_set: ./data/mt_bench/training_set.jsonl
  valid_set: ./data/mt_bench/valid_set.jsonl
  test_set: ./data/mt_bench/test_set.jsonl

# Model paths
model_path:
  checkpoint_root: ./models
  save_model_path: ./models/gmtrouter_checkpoint.pt
  load_model_path: ./models/gmtrouter_checkpoint.pt

# Data Download Instructions:
# ===========================
# 1. Download GMTRouter dataset from Google Drive
#    Link: https://drive.google.com/file/d/[GMTRouter_dataset_id]
#
# 2. Extract the archive:
#    tar -xzvf GMTRouter_dataset.tar.gz
#
# 3. Move data folder to repository root:
#    mv data ./
#
# 4. Data structure:
#    ./data/
#      ├── chatbot_arena/
#      │   ├── training_set.jsonl
#      │   ├── valid_set.jsonl
#      │   └── test_set.jsonl
#      ├── gsm8k/
#      │   ├── training_set.jsonl
#      │   ├── valid_set.jsonl
#      │   └── test_set.jsonl
#      ├── mmlu/
#      │   └── ...
#      └── mt_bench/
#          └── ...
#
# Data Format (JSONL):
# ====================
# Each line is a JSON object with:
# {
#   "judge": "user_id",                           # User identifier
#   "model": "gpt-4",                             # LLM model name
#   "question_id": "12345",                       # Question identifier
#   "turn": 1,                                    # Turn number
#   "conversation": [
#     {
#       "query": "What is machine learning?",     # Query text
#       "query_emb": [0.1, 0.2, ...],            # Query embedding vector
#       "response": "Machine learning is...",     # Response text (optional)
#       "rating": 4.5                             # Quality rating
#     }
#   ],
#   "model_emb": [0.3, 0.4, ...],                # LLM embedding vector
#   "encoder": "sentence-transformers/all-mpnet-base-v2"  # PLM model name
# }

# Training Instructions:
# ======================
# For actual training, use the original GMTRouter repository:
#
# 1. Clone the repository:
#    git clone https://github.com/ulab-uiuc/GMTRouter
#    cd GMTRouter
#
# 2. Setup environment:
#    conda create -n gmtrouter python=3.11.13
#    conda activate gmtrouter
#    pip install torch==2.6.* --index-url https://download.pytorch.org/whl/cu124
#    pip install -r requirements.txt
#    pip install torch-geometric==2.6.1
#
# 3. Download and prepare data (see above)
#
# 4. Run training:
#    python src/train.py --config configs/sample.yaml
#
# 5. Copy trained checkpoint to LLMRouter:
#    cp models/[checkpoint].pt [LLMRouter_path]/models/gmtrouter_checkpoint.pt
