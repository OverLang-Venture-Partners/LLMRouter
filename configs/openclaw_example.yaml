# ============================================================
# OpenClaw Router Configuration Example
# ============================================================
# Start: llmrouter serve --config configs/openclaw_example.yaml
# ============================================================

# Server settings
serve:
  host: "0.0.0.0"
  port: 8000
  show_model_prefix: true  # Add [model_name] prefix to responses

# ============================================================
# Router Strategy Configuration
# ============================================================
# Available strategies:
#   - random: Random model selection (with optional weights)
#   - round_robin: Rotate through models
#   - rules: Keyword-based routing
#   - llm: Use an LLM to decide
#   - llmrouter: Use LLMRouter ML-based routers
#
# For llmrouter strategy, supported routers include:
#   - randomrouter, thresholdrouter (custom_routers/)
#   - knnrouter, mlprouter, svmrouter, mfrouter, elorouter
#   - dcrouter, graphrouter, causallmrouter, gmtrouter
#   - and more...
#
# All strategies work with Bedrock models! Mix Bedrock and non-Bedrock
# providers in the same configuration for optimal cost/performance.
# ============================================================

router:
  # ========== Random Strategy ==========
  # Example: Mix Bedrock and NVIDIA models with weights
  strategy: random
  weights:
    llama-3.1-8b: 3          # NVIDIA model (fast, low cost)
    llama3-70b: 1            # NVIDIA model (powerful)
    # claude-3-sonnet: 2     # Bedrock model (balanced)
    # titan-text-express: 1  # Bedrock model (cost-effective)

  # ========== Rules Strategy ==========
  # Example: Route by keywords to different providers
  # strategy: rules
  # rules:
  #   - keywords: ["code", "python", "javascript", "function"]
  #     model: llama-3.3-nemotron-super-49b-v1  # NVIDIA for coding
  #   - keywords: ["analyze", "reasoning", "complex"]
  #     model: claude-3-sonnet  # Bedrock Claude for reasoning
  #   - keywords: ["hello", "hi", "thanks"]
  #     model: titan-text-express  # Bedrock Titan for simple queries
  #   - default: llama-3.1-8b  # NVIDIA as default

  # ========== LLM Strategy ==========
  # strategy: llm
  # provider: nvidia
  # model: meta/llama-3.1-8b-instruct
  # base_url: https://integrate.api.nvidia.com/v1

  # ========== LLMRouter Strategy (ML-based) ==========
  # Example: Use ML-based router with mixed providers
  # strategy: llmrouter
  # llmrouter:
  #   name: knnrouter  # or: thresholdrouter, mlprouter, etc.
  #   config_path: configs/model_config_train/knnrouter.yaml
  #   model_path: saved_models/knnrouter.pt  # optional
  # 
  # Note: ML routers can route to both Bedrock and non-Bedrock models
  # based on learned patterns from training data

# ============================================================
# API Keys
# ============================================================
api_keys:
  nvidia:
    - nvapi-xxx...  # Replace with your NVIDIA API key(s)
  openai: ${OPENAI_API_KEY}  # Supports environment variables
  anthropic: ${ANTHROPIC_API_KEY}

# ============================================================
# AWS Credentials (for Bedrock models)
# ============================================================
# AWS Bedrock uses standard AWS credential mechanisms (not API keys).
# Configure credentials using one of these methods:
#
# 1. Environment variables (recommended for development):
#    export AWS_ACCESS_KEY_ID="your-access-key"
#    export AWS_SECRET_ACCESS_KEY="your-secret-key"
#    export AWS_DEFAULT_REGION="us-east-1"
#
# 2. AWS credential file (~/.aws/credentials):
#    [default]
#    aws_access_key_id = your-access-key
#    aws_secret_access_key = your-secret-key
#
#    And ~/.aws/config:
#    [default]
#    region = us-east-1
#
# 3. IAM role (automatic when running on AWS infrastructure like EC2, ECS, Lambda)
#
# Note: Each Bedrock model can specify its own aws_region to override the default.
#
# OpenClaw automatically detects Bedrock providers and routes requests
# appropriately - no additional configuration needed!
# ============================================================

# ============================================================
# Optional: Routing Memory (Retrieval-Augmented Routing)
# ============================================================
# When enabled, OpenClaw Router persists (query -> selected model) pairs to disk
# and retrieves top-k similar past queries to help the `llm` routing strategy.
memory:
  enabled: false
  # If omitted/empty, defaults to: ~/.llmrouter/openclaw_memory.jsonl
  # path: "${HOME}/.llmrouter/openclaw_memory.jsonl"
  top_k: 10
  retriever_model: "facebook/contriever-msmarco"
  device: "cpu"
  max_length: 256
  max_query_chars: 500
  max_prompt_chars: 200
  per_user: false

# ============================================================
# LLM Backend Configuration
# ============================================================
llms:
  # Fast models
  llama-3.1-8b:
    provider: nvidia
    model: meta/llama-3.1-8b-instruct
    base_url: https://integrate.api.nvidia.com/v1
    description: "Fast responses, daily chat"
    max_tokens: 1024
    context_limit: 128000
    input_price: 0.2
    output_price: 0.2

  # Strong models
  llama3-70b:
    provider: nvidia
    model: meta/llama3-70b-instruct
    base_url: https://integrate.api.nvidia.com/v1
    description: "Complex reasoning, deep analysis"
    max_tokens: 1024
    context_limit: 8192
    input_price: 0.9
    output_price: 0.9

  llama-3.3-nemotron-super-49b-v1:
    provider: nvidia
    model: nvidia/llama-3.3-nemotron-super-49b-v1
    base_url: https://integrate.api.nvidia.com/v1
    description: "Code generation, technical Q&A"
    max_tokens: 1024
    context_limit: 32768
    input_price: 0.9
    output_price: 0.9

  # Instruction models
  mistral-7b:
    provider: nvidia
    model: mistralai/mistral-7b-instruct-v0.3
    base_url: https://integrate.api.nvidia.com/v1
    description: "Instruction following, structured output"
    max_tokens: 1024
    context_limit: 32768
    input_price: 0.2
    output_price: 0.2

  # ============================================================
  # AWS Bedrock Models
  # ============================================================
  # Bedrock models use AWS credentials (see AWS Credentials section above)
  # Each model can specify an aws_region to override the default region
  # 
  # âœ… FULLY SUPPORTED: Bedrock models work at runtime with OpenClaw server!
  # - Synchronous and streaming responses
  # - WebSocket support
  # - All routing strategies (random, round_robin, rules, llm, llmrouter)
  # - Mixed with non-Bedrock providers in same configuration
  # ============================================================

  # Claude models (Anthropic via Bedrock)
  claude-3-sonnet:
    provider: bedrock
    model: anthropic.claude-3-sonnet-20240229-v1:0
    description: "Claude 3 Sonnet via AWS Bedrock - balanced performance and speed"
    max_tokens: 4096
    context_limit: 200000
    input_price: 3.0
    output_price: 15.0
    aws_region: us-east-1  # Optional: override default region

  claude-3-haiku:
    provider: bedrock
    model: anthropic.claude-3-haiku-20240307-v1:0
    description: "Claude 3 Haiku via AWS Bedrock - fast and cost-effective"
    max_tokens: 4096
    context_limit: 200000
    input_price: 0.25
    output_price: 1.25
    aws_region: us-west-2  # Example: different region for this model

  # Amazon Titan models
  titan-text-express:
    provider: bedrock
    model: amazon.titan-text-express-v1
    description: "Amazon Titan Text Express - fast and cost-effective text generation"
    max_tokens: 8192
    context_limit: 8192
    input_price: 0.2
    output_price: 0.6
    aws_region: us-east-1

  # Meta Llama models via Bedrock
  llama3-70b-bedrock:
    provider: bedrock
    model: meta.llama3-70b-instruct-v1:0
    description: "Llama 3 70B via AWS Bedrock - powerful open model"
    max_tokens: 2048
    context_limit: 8192
    input_price: 2.65
    output_price: 3.5
    aws_region: us-west-2

  # Mistral models via Bedrock
  mistral-7b-bedrock:
    provider: bedrock
    model: mistral.mistral-7b-instruct-v0:2
    description: "Mistral 7B via AWS Bedrock - efficient instruction following"
    max_tokens: 8192
    context_limit: 32768
    input_price: 0.15
    output_price: 0.2
    aws_region: eu-west-1  # Example: European region


# ============================================================
# OpenClaw Integration
# ============================================================
# Add to ~/.openclaw/openclaw.json:
#
# {
#   "models": {
#     "providers": {
#       "openclaw": {
#         "api": "openai-completions",
#         "baseUrl": "http://localhost:8000/v1",
#         "apiKey": "not-needed",
#         "models": [{"id": "auto", "name": "OpenClaw Router"}]
#       }
#     }
#   },
#   "agents": {
#     "defaults": {
#       "model": {"primary": "openclaw/auto"}
#     }
#   }
# }
